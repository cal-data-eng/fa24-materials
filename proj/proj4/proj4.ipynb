{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"proj4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Mongo \n",
    "\n",
    "## Due Date: Wed 11/20, 5:00 PM\n",
    "\n",
    "In this project, we will be investigating how different database systems handle semi-structured JSON data. In particular, we will be placing emphasis on the use of MongoDB: a database system that stores data in a construct known as documents. These documents are very similar to the JSON objects we've explored in lecture, with a few differences in representation and indexing that we will explore in the following questions. \n",
    "\n",
    "In this project, we will be working with the **Yelp Academic Dataset** which contains the datasets `businesses`, `reviews`, and `users`. Due to the limitations of JupyterHub and the Mongo instances we are working with, `reviews` and `users` are truncated to 7500 reviews and 1000 users. We will be using the full `businesses` dataset, however.\n",
    "\n",
    "Throughout the course of this project, you should understand what Mongo can (and cannot) do with regards to its documents as a NoSQL datastore, and compare and contrast this to other data representation formats such as the relational model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics & Scoring Breakdown\n",
    "\n",
    "Please read the submission instructions carefully and double check that your submission is not throwing any errors. Please ensure that public tests pass upon submission. It is your responsibility to wait until the autograder finishes running. We will not be accepting regrade requests for submission issues.\n",
    "\n",
    "Each coding question has **both public tests and hidden tests**. Roughly 50% of your coding grade will be made up of your score on the public tests released to you, while the remaining 50% will be made up of unreleased hidden tests. **Free-response questions (marked 'm' in the table below) are manually graded.**\n",
    "\n",
    "This is an **individual project**. However, you’re welcome to collaborate with any other student in the class as long as it’s within the academic honesty guidelines.\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "1a\t| 1\n",
    "1b  | 1\n",
    "1c\t| 2\n",
    "1d\t| 1\n",
    "1e\t| 2\n",
    "1f  | 1\n",
    "2a\t| m: 2\n",
    "2b\t| 1\n",
    "2c  | 1\n",
    "2d  | m: 2\n",
    "3a\t| m: 1\n",
    "3b\t| 1\n",
    "3c\t| 1\n",
    "3d  | 1\n",
    "3e  | 1\n",
    "3f  | 3\n",
    "4a\t| 1\n",
    "4b\t| 2\n",
    "4c\t| 2\n",
    "4d  | 1\n",
    "**Total** | 28\n",
    "\n",
    "**Grand Total:** 28 points (autograded: 23, manual: 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Up Mongo\n",
    "We will be using [PyMongo](https://www.mongodb.com/docs/languages/python/pymongo-driver/current/), a Python wrapper for MongoDB, for this project. **Note that the method names and syntax in PyMongo might be slightly different compared to pure MongoDB, for example `update_one` instead of `updateOne`.**\n",
    "\n",
    "Every student should have access to their own MongoDB instance, running on the localhost of your Datahub server. After running the following cell, for the rest of the project, you can use the Python variables `business`, `review`, and `user` to access the corresponding collection.\n",
    "\n",
    "To prevent bracket mismatches while creating your queries, it is recommended to **turn on \"Auto Close Brackets\"** via Settings (top left) in JupyterHub.\n",
    "\n",
    "Furthermore, since we are using Python dictionaries as our query filter, make sure to wrap all keys and values inside quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import TEXT\n",
    "import numpy as np\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost\")\n",
    "mydb = myclient[\"yelp\"]\n",
    "business = mydb[\"business\"]\n",
    "review = mydb[\"review\"]\n",
    "user = mydb[\"user\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**PLEASE READ:** Please avoid printing too many debugging query outputs—it may crash your JupyterHub if your file size becomes too large! It's recommended to use the `limit()` method and delete any debugging query cells if no longer needed as you go through the project.\n",
    "\n",
    "You might run into issues on the project where you are certain your code works but the output is incorrect. This may be because your collections have been corrupted. Run the following cell and uncomment the specific collections you would like to drop if you would like to remake your collections from scratch. **Be sure to re-run the Load Datasets cells below if you drop your collections so you aren't working with empty collections!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN THIS CELL IF YOU WOULD LIKE TO REMAKE YOUR COLLECTIONS FROM SCRATCH. \n",
    "# IF YOU DROP ANY COLLECTIONS, RE-RUN THE NEXT TWO CELLS TO LOAD IN THE DATA.\n",
    "\n",
    "# review.drop()\n",
    "# business.drop()\n",
    "# user.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "The following two cells will load the JSON datasets into the appropriate Mongo collections. You will only need to run them once unless you drop the collections above. The second cell **may take a couple of minutes to run** if you are running it for the first time or are running it after you dropped the collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os.path\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_review.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_review.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_user.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_user.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_business.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_business.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL MAY TAKE AT MOST 5 MINUTES. BUT HOPEFULLY YOU WILL ONLY NEED TO RUN IT ONCE.\n",
    "import json\n",
    "\n",
    "if business.count_documents({}) == 0:\n",
    "    print(\"Loading business collection...\")\n",
    "    with open('data/yelp_academic_dataset_business.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            business.insert_one(json.loads(line))\n",
    "\n",
    "if review.count_documents({}) == 0:\n",
    "    print(\"Loading review collection...\")\n",
    "    with open('data/yelp_academic_dataset_review.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            review.insert_one(json.loads(line))\n",
    "            \n",
    "if user.count_documents({}) == 0:\n",
    "    print(\"Loading user collection...\")\n",
    "    with open('data/yelp_academic_dataset_user.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            user.insert_one(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our collections. For the command below, replace `user` with `review` or `business` to count the number of documents in each collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect our collections. The code below retrieves the first document in `business`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a document containing a business named `Oskar Blues Taproom` when you run the command above, it means that our JSON data has successfully been imported into the collection!\n",
    "\n",
    "You should be able to similarly view the first document in `user` and `review` by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming there were no errors, now we can get started with exploring Mongo in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run the following cell, no further action is needed.\n",
    "from data101_utils import GradingUtil\n",
    "grading_util = GradingUtil(\"proj4\")\n",
    "grading_util.prepare_autograder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Basic MQL\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "In lecture, we discussed how one could find specific attributes from a JSON object using dot (`.`) notation. \n",
    "\n",
    "- While you can still use the dot notation in queries, PyMongo represents documents returned from Mongo queries using Python dictionaries, making it convenient to manipulate JSON using a mix of Mongo queries and array indexing.\n",
    "    - Specifically, given the result of a retrieval `find` query, you can look up the third document by indexing with `[2]`. Note, since we are using Python arrays, we will be using 0-based indexing. Then, given this document, you can look up the field `'amount'` by appending `['amount']` etc., adding multiple square brackets as needed to \"walk down\" the JSON tree representation via `collection.find(...)[2]['amount']`. This will return the `'amount'` field from the 3rd document returned from the query. This combination of query and indexing will be useful in obtaining the necessary information you need for this and other questions.\n",
    "- In order to get a visual output of the query results, you will need to wrap `collection.find(...)` inside `list()`, e.g. `list(collection.find(...))`. This is because `collection.find(...)` returns a **Cursor** object, which is an iterator. **An important consequence** is that if we set `result = collection.find(...)`, then calling `list(result)` for the first time will get you the expected list of documents in the query result, but calling `list(result)` for a second time will give you an empty list! So wrapping `collection.find(...)` directly inside `list()` would avoid this issue. With that in mind, you may not *always* need to obtain a visual output of the results.\n",
    "- Be aware of the distinction of when you are querying with Mongo versus Python-based array indexing into your Mongo query results (i.e. you are wrapping your query inside `list()` and *then* indexing into that list.)\n",
    "- **As a reminder, since we are using Python dictionaries as our query filter, make sure to wrap all keys (and applicable values) inside quotes. Otherwise, errors will occur.**\n",
    "\n",
    "As a warmup to get you familiarized with PyMongo syntax in combination with Python-based array indexing, find the **Tuesday hours** for the restaurant named **Legal Sea Foods** at **100 Huntington Ave** in **Boston**. Be careful—there are many Legal Sea Foods in Boston!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_1a = ...\n",
    "result_1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_1a\", result_1a);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b\n",
    "Now, let's get some practice with aggregation and filtering. Our goal is to write a query that computes the average star rating for all businesses in Colorado with 30 reviews or greater. However, this won't be as easy as setting the state to CO! If we inspect this dataset more closely, we will notice that some cities are not matched up with the right states. As an example, run the query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(business.find({\"state\": \"CA\"}).limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how cities like Portland, Atlanta, and Orlando are classified as California cities! However, the latitude and longitude is generally correct. The latitude of Colorado is between 37 and 41 **inclusive** and the longitude is between -109 and -102 **inclusive**. Now, use this to **find the average star rating** of all businesses in this range with **30 or more reviews**.\n",
    "\n",
    "Recall that in SQL, we would use a `GROUP BY` with the `AVG` aggregation function. In Mongo, we use an aggregation pipeline [(documentation here)](https://www.mongodb.com/docs/manual/reference/method/db.collection.aggregate/), comprised of multiple stages (e.g., `$match` followed by `$group`). Each stage transforms the documents in some way. Pipeline stages do not need to produce one output document for every input document. For example, some stages may generate new documents or filter out documents.\n",
    "\n",
    "**Hints:**\n",
    "- As in the previous question, you may find it helpful to use the PyMongo array notation to extract the pertinent information/document once you have composed the right Mongo aggregation query. You are required to wrap `collection.aggregate(...)` inside `list()`, e.g. `list(collection.aggregate(...))` before indexing/visualizing the output. Similar to `collection.find(...)`, `collection.aggregate(...)` also returns a **Cursor** object (which is an iterator).\n",
    "\n",
    "- You can set multiple conditions for a given field within the same object, e.g. `{\"$gte\": 0, \"$lte\": 10}`. This is the recommended approach, or else you may need to worry about the ordering between the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_1b = ...\n",
    "result_1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_1b\", result_1b);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1c\n",
    "\n",
    "In this question, we will explore aggregation and grouping further. We will also make use of the `$project` operator which allows us to output documents with certain fields of our choosing. \n",
    "\n",
    "For this question, we would like to create an aggregation pipeline to find the town in each state with the highest average number of stars. **We will only consider towns with greater than or equal to 5 reviews in total across all the businesses in that town so that the average is meaningful (i.e. there should be at least 5 reviews in that city).** Your final output should contain exactly two fields:\n",
    "\n",
    "- `average_stars` which contains the average number of stars for the corresponding town.\n",
    "- `city_state` which is the name of the town with the highest value of average stars in the state concatenated with a comma followed by the state initials. For example, `Berkeley, CA`.\n",
    "\n",
    "\n",
    "To ensure your output is consistent with the autograder, **sort in descending order by `average_stars` and break ties by sorting on `city_state` in alphabetical (ascending) order.** This is real-world data and may be messy and filled with typos, including your output. However, there is no need to clean the data. You may also ignore upper and lower case discrepancies.\n",
    "\n",
    "As a concrete example, imagine that Berkeley and Austin have the highest average stars in California and Texas respectively (and both have more than or equal to 5 total reviews in this *truncated* dataset). If Berkeley and Austin both have an average star rating of 5.0, your final output should be:\n",
    "\n",
    "```\n",
    "[\n",
    "    {'average_stars': 5.0, 'city_state': 'Austin, TX'},\n",
    "    {'average_stars': 5.0, 'city_state': 'Berkeley, CA'}\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "**Note:** You will provide a pipeline to `business.aggregate(...)` as your solution. Save your pipeline to `q1c_pipeline`.\n",
    "\n",
    "**Hint:** You may find the `concat` operator helpful [(documentation here)](https://docs.mongodb.com/manual/reference/operator/aggregation/concat/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q1c_pipeline = ...\n",
    "\n",
    "result_1c = list(business.aggregate(q1c_pipeline))\n",
    "result_1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_1c\", result_1c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "In class, we've described structured (rectangular) data as well as semi-structured data. We haven't quite covered unstructured data—this is basically free-form text. Often, in semi-structured JSON you may have unstructured text data embedded within, such as the text field in the review collection.\n",
    "\n",
    "MongoDB allows us to build a so-called **text index** to retrieve the relevant document based on keywords found in text in a predefined field. This index converts our free-form text into a structure that allows us to easily look up documents by its contents. To leverage this text search capability, we build a text index on the `text` field in the `review` collection. This has been done for you.\n",
    "\n",
    "We will then use this text index to do basic sentiment analysis and find all the restaurants we should avoid! Using the text index given, write a query to find all the reviews with \"disgusting\", \"horrible\", \"horrid\", \"gross\", \"bad\", or \"hate\". To use the text index, use the keywords `$text` and `$search` as detailed [here](https://www.mongodb.com/docs/manual/core/text-search-operators/).\n",
    "\n",
    "Fill in your query into `result_1d` to count how many reviews contain any of these 6 words.\n",
    "\n",
    "**Hint:** In general, you can count the number of documents returned by a `find` query result via `len(list(collection.find(...)))` or more simply `collection.count_documents(...)`. To count the number of documents returned by an `aggregate` query result, the best way is to directly use `len(list(collection.aggregate(...)))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create a text index here\n",
    "if 'text_text' not in review.index_information():\n",
    "    review.create_index([('text', TEXT)])\n",
    "\n",
    "result_1d = ...\n",
    "result_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_1d\", result_1d);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1e\n",
    "\n",
    "Now, let's learn Mongo updates, deletions, and creation. Create a new collection called `review_boolean` which is the exact same as `reviews` EXCEPT there is a new field called `to_avoid` which is the ***string*** `\"true\"`  if the review `text` contains the words \"disgusting\", \"horrid\", \"horrible\", \"gross\", \"bad\", or \"hate\" and the ***string*** `\"false\"` if not.  \n",
    "\n",
    "This is a tricky task! We have not discussed creation, updates, or insertions in great detail during lecture but luckily, Mongo uses a similar approach to SQL.\n",
    "\n",
    "***Insertions***: In order to insert into a document, you may use the functions [review_boolean.insert_one(...)](https://docs.mongodb.com/manual/reference/method/db.collection.insertOne/) or [review_boolean.insert_many(...)](https://docs.mongodb.com/manual/reference/method/db.collection.insertMany/). These functions take in a document or a list of documents and inserts them into the collection. \n",
    "\n",
    "***Updates***: In order to update a document, you may use the functions [review_boolean.update_one(...)](https://docs.mongodb.com/manual/reference/method/db.collection.updateOne/) or [review_boolean.update_many(...)](https://docs.mongodb.com/manual/reference/method/db.collection.updateMany/). These functions take in two parameters.\n",
    "1. The first parameter specifies which documents should be modified. For `update_many`, if the first parameter is `{}`, this indicates that all documents should be updated. However, you can add a more specific filter here if you would like.\n",
    "2. The second parameter specifies what you would like to update your field to (the [$set](https://docs.mongodb.com/manual/reference/operator/update/set/) operator may come in handy here). Recall that in our SQL model, updates are performed as `UPDATE ... SET ... WHERE ...`. In our case, the first ellipsis corresponds to `review_boolean`, the second ellipsis corresponds to the second parameter of `update_*` where `*` can be `one` or `many`, and the third ellipsis corresponds to the first parameter of `update_*`.\n",
    "\n",
    "***Creation***: We handle creation of the collection for you. But in PyMongo, creation of a collection is as simple as writing `variable_name = db[collection_name]` where `db` is the the PyMongo database object variable you have already created.\n",
    "\n",
    "Some additional reminders and hints:\n",
    "- The empty collection `review_boolean` has already been created for you and is stored in the variable of the same name.\n",
    "- A text index has been created for you. You can use a similar search approach as the last question.\n",
    "- We want to start by inserting the documents from the `review` collection into the `review_boolean` collection.\n",
    "- Don't forget that in order to pass the hidden tests, the `to_avoid` field must exist for every document in `review_boolean`! The [$exists](https://www.mongodb.com/docs/manual/reference/operator/query/exists/) operator may be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_boolean = mydb[\"review_boolean\"]\n",
    "review_boolean.drop()\n",
    "\n",
    "# We create a text index here\n",
    "if 'text_text' not in review_boolean.index_information():\n",
    "    review_boolean.create_index([('text', TEXT)])\n",
    "\n",
    "# YOUR ANSWER BEGINS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_boolean = mydb[\"review_boolean\"]\n",
    "review_boolean.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "review_boolean = mydb[\"review_boolean\"]\n",
    "grading_util.save_results(\"result_1e\", list(review_boolean.find({}, {'_id': 0})));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1f\n",
    "\n",
    "Now, you have a change of heart: you decide that it's unfair to label restaurants as `to_avoid` without at least giving them a chance! Remove the `to_avoid` field from the `review_boolean` collection. Calculate the `difference` between the data size of `review_boolean` with the `to_avoid` field and without it. The code for making this calculation is provided but it is up to you to actually remove the field.\n",
    "\n",
    "***Deletions***: Deletions in Mongo make use of the `review_boolean.update_one(...)` or `review_boolean.update_many(...)` functionality discussed in Question 1e. However, this time, instead of using the `$set` operator which allows for the creation of new fields, we will use the [$unset](https://docs.mongodb.com/manual/reference/operator/update/unset/) operator which deletes them! (Very tidy!)\n",
    "\n",
    "**Before running the next cell, make sure to re-run your cell for Question 1e so you don't get a difference of 0!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with_avoid = mydb.command(\"collstats\", \"review_boolean\")['size']\n",
    "\n",
    "# YOUR ANSWER BEGINS HERE\n",
    "# END\n",
    "\n",
    "without_avoid = mydb.command(\"collstats\", \"review_boolean\")['size']\n",
    "difference = with_avoid - without_avoid\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_1f\", difference);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: JSON and Relational Models\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Now we have a good idea of how to do retrieval, aggregation, and updates in Mongo. But, we haven't talked about why we\n",
    "would want to use Mongo to store JSON! In order to explore this, let's take another look at the `business`\n",
    "collection. We will look at the first two entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(business.find({}).limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "What are **two** benefits of storing this data in MongoDB with JSON over a relational database management system such as Postgres?\n",
    "Please reference specific examples from the `business` collection to back up your claims. \n",
    "\n",
    "Format your answer as follows:\n",
    "\n",
    "1. Benefit #1, Example #1.\n",
    "2. Benefit #2, Example #2.\n",
    "\n",
    "**Limit each benefit to one sentence and each example to one sentence for a total of at most four sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2b\n",
    "\n",
    "It seems like MongoDB is getting all the love when it comes to JSON support! However, modern iterations of relational databases\n",
    "such as Postgres 9.3+ also have [excellent JSON functionality](https://www.postgresql.org/docs/9.3/functions-json.html) as we will soon explore in this task. First, let's set up a\n",
    "bit of scaffolding. The following cell will import the `yelp_academic_dataset_review.json` data into a table called `reviews` in the Postgres Yelp database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%sql postgresql://jovyan@127.0.0.1:5432/postgres\n",
    "\n",
    "!psql -h localhost -c 'DROP DATABASE IF EXISTS yelp'\n",
    "!psql -h localhost -c 'CREATE DATABASE yelp'\n",
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS reviews'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE reviews(data TEXT);'\n",
    "!cat data/yelp_academic_dataset_review.json | psql -h localhost -d yelp -c \"COPY reviews (data) FROM STDIN;\"\n",
    "%sql \\l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to connect to the Postgres Yelp database. There should be no errors after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://jovyan@127.0.0.1:5432/yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to observe how this new `reviews` table looks. Note that the `data` column is stored as TEXT and not as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM reviews LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the reviews table consists of one column named `data`. This column contains all the JSON documents in the \n",
    "reviews collection *in text format*. Use [Postgres' JSON functions](https://www.postgresql.org/docs/9.3/functions-json.html) to write a query that converts the JSON object fields into their own `TEXT` columns. (**Hint:** One of the operators in Table 9-40 may be useful). To be more concrete, your query should contain 8 columns in this particular order:\n",
    "\n",
    "| review_id | user_id | business_id | stars | useful | funny | cool | text |\n",
    "| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n",
    "\n",
    "Each row should correspond to one JSON document. Some skeleton code (that does the mundane work of converting data to JSON properly) is provided to you. You will only need to fill in the `SELECT` clause.\n",
    "\n",
    "Hint: The `values` column is what stores the JSON data for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql --save query_2b result_2b <<\n",
    "...\n",
    "FROM (SELECT CAST(regexp_replace(data, E'[\\\\n\\\\r]+', '','g') AS JSON) AS values FROM reviews) AS b\n",
    "ORDER BY review_id\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "query_2b = %sqlcmd snippets query_2b\n",
    "grading_util.save_results(\"result_2b\", query_2b, result_2b)\n",
    "result_2b.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2c\n",
    "\n",
    "One important aspect of data engineering that we have not referred to yet is joins. We have seen that through the use of indices, selection/projection predicate pushdowns, and various physical implementations (as well as orderings) that joins can be done quite efficiently in relational SQL based databases. How do joins fare in Mongo where the data stored is inherently semi-structured? Let's investigate! For this question, we have provided you access to the tables `business_complete` and `review_complete` which contain the collections `business` and `review`  in relational form as described in Question 2b (the columns of the relations\n",
    "are fields in the JSON document). Each relation has its respective `id` (`business_id` or `review_id`) column as its primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS business_complete'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE business_complete(business_id TEXT PRIMARY KEY, name TEXT, address TEXT, city TEXT, state TEXT, postal_code TEXT, latitude TEXT,longitude TEXT, stars TEXT, review_count TEXT, is_open TEXT, attributes TEXT, categories TEXT, hours TEXT);'\n",
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS review_complete'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE review_complete(review_id TEXT PRIMARY KEY, user_id TEXT, business_id TEXT, stars TEXT, useful TEXT, funny TEXT, cool TEXT,text TEXT);'\n",
    "!cat data/business.csv | psql -h localhost -d yelp -c \"COPY business_complete (business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,is_open,attributes,categories,hours) FROM STDIN CSV HEADER;\"\n",
    "!cat data/review.csv | psql -h localhost -d yelp -c \"COPY review_complete (review_id, user_id, business_id, stars, useful, funny, cool, text) FROM STDIN CSV HEADER;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how `review_complete` looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM review_complete LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this current moment in time, Mongo only supports left joins (via the lookup aggregation stage). This is what we will compare against SQL.\n",
    "\n",
    "Let's start by writing a SQL query (as a Python string below) that displays all the reviews along with their associated business information. You should perform a **left join** between the `review_complete` table and the `business_complete` table on the `business_id` column, and then project all columns. Keep a mental note of the **execution time** that you see in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_2c_str = ...\n",
    "!psql -h localhost -d yelp -c \"explain analyze $result_2c_str\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now, let's perform the equivalent left join in Mongo between `review` and `business`. **The output array field should be named as `business_info`**. Feel free to refer to the `$lookup` [documentation](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/).\n",
    "\n",
    "**Note:** You will provide a single-stage pipeline to `review.aggregate(...)` as your solution. Save your pipeline to `q2c_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We first create an index on business_id in the business collection\n",
    "business.create_index('business_id', unique=True)\n",
    "\n",
    "q2c_pipeline = ...\n",
    "\n",
    "result_2c = list(review.aggregate(q2c_pipeline))[:5]\n",
    "# Uncomment the line below to see your output\n",
    "# result_2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "result_2c = list(review.aggregate(q2c_pipeline))[:5]\n",
    "grading_util.save_results(\"result_2c\", result_2c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to examine the query plan for the Mongo query that you just wrote. Again, make a mental note of the execution time that you see. (Look at the value corresponding to the key `executionTimeMillis`, **NOT TO BE CONFUSED WITH `executionTimeMillisEstimate`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb.command('explain', {'aggregate': 'review', 'pipeline': q2c_pipeline, 'cursor': {}}, verbosity='executionStats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2d\n",
    "\n",
    "In the last question, you performed equivalent left joins in both Postgres and Mongo. Now, examine their query plans, paying special attention to `executionTimeMillis`. Which database system was faster? What gives the database system you chose an advantage over the other? Keep your response to at most three sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Dataframes / Pandas\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "So far, we've talked about NoSQL / document databases like Mongo and relational databases like Postgres. Now, we will explore data transformation with a different data model: dataframes. Dataframes are similar to relations; we will dive into the differences here. To this end, we will use Pandas, a Python package that allows us to work with dataframes. Pandas is widely adopted by data scientists for data loading, wrangling, cleaning, and analysis. To start, let us export our MongoDB collections into Pandas using a function called `json_normalize`. We need to truncate\n",
    "`business` before we can use it to meet the memory constraints set by Jupyter. The variable `business_trunc` will contain the reference to the truncated business collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_trunc = mydb[\"business_trunc\"]\n",
    "business_trunc.drop()\n",
    "count = 0\n",
    "for document in business.find({}):\n",
    "    business_trunc.insert_one(document)\n",
    "    count += 1\n",
    "    if count == 1000:\n",
    "        break\n",
    "\n",
    "business_cursor = business_trunc.find({})\n",
    "review_cursor = mydb[\"reviews\"].find({})\n",
    "user_cursor = mydb[\"users\"].find({})\n",
    "\n",
    "# Load the collections into Pandas. \n",
    "from pandas import json_normalize\n",
    "user_df = json_normalize(user_cursor)\n",
    "review_df = json_normalize(review_cursor)\n",
    "business_df = json_normalize(business_cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of Question 3, please use the three dataframes we just created: `user_df`, `review_df`, and `business_df`. Let's take a look at the first five rows of `business_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "1. What do you notice about how the columns of `business_df` are constructed, e.g. how are fields and subfields represented in the dataframe?\n",
    "2. How are values that are not found in every document handled in the pandas dataframe?\n",
    "\n",
    "Hint: You will need to horizontally scroll to view all the column names and values. \n",
    "\n",
    "**Format your answer as follows and use one sentence per question:**\n",
    "\n",
    "1. Sentence 1\n",
    "2. Sentence 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3b\n",
    "\n",
    "In the previous question, we talked about how Mongo and Postgres approach joins. Pandas is also capable of performing joins using the [merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function! For this task, perform an inner join on `business_df` with itself on the column `stars`. The final dataframe should be saved to a variable called `result_3b` and should only contain 3 columns in this particular order: the name of the first restaurant, the name of the second restaurant, and the number of the stars. Your dataframe header should look like this:\n",
    "\n",
    "| name_x | name_y | stars |\n",
    "| :--- | :--- | :--- |\n",
    "\n",
    "Note that the `_x` and `_y` get appended automatically by Pandas during the `merge`. You don't have to do anything special to achieve this.\n",
    "\n",
    "**Hint:** Check out [this tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html) on selecting a subset of the dataframe. This will be helpful in the rest of Question 3 as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_3b = ...\n",
    "result_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_3b\", result_3b.sort_values(['name_x', 'name_y', 'stars'])[:50]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3c\n",
    "\n",
    "Due to the nested representation of the data, there are a lot of missing fields with `NaN` values in the `business_df` dataframe as you may have noticed in Question 3a. Construct a dataframe `missing_value_df` with two columns: `column_name` and `percent_missing`. `percent_missing` should be the percentage of `NaN` values in the corresponding column in `business_df`. \n",
    "\n",
    "For example, if 25% of values are `NaN` in the `name` column, then `percent_missing` would have the value `25.0` (***not*** 0.25).\n",
    "\n",
    "Your dataframe header should look like this:\n",
    "\n",
    "| column_name | percent_missing |\n",
    "| :--- | :--- |\n",
    "\n",
    "**Hint:** Use Pandas' [isnull](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html) function followed by [sum](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_value_df = pd.DataFrame({'column_name': business_df.columns,\n",
    "                                 ...\n",
    "missing_value_df.reset_index(drop=True, inplace=True)\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_3c\", missing_value_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3d\n",
    "\n",
    "Plot a histogram distribution of the percentage of NaN values across all columns (via Pandas' [hist](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) function). Don't worry about adding titles / making it look nice—we won't be grading the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examine the histogram that you just plotted. How many columns are 90%+ `NaN`? Input your answer into `result_q3d` as an integer (e.g. if your answer is 6, then **explicitly** declare/hard-code `result_q3d = 6`. Do not refer to any values in the dataframe; self-created dataframes are not saved in the export file, so any references to them will not work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_q3d = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3e\n",
    "\n",
    "Let us now alter `business_df` to exclude the columns with more than 80%+ `NaN` values (keeping columns with 80% `NaN` values or less). This likely means the corresponding attributes are not an important factor for most businesses so we can get rid of them in our `business_df`. Create a new dataframe called `important_attribute_business_df` which only contains these columns.\n",
    "\n",
    "**Hint:** Check out [this section](https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html#how-do-i-select-specific-rows-and-columns-from-a-dataframe) from the tutorial linked in Question 3b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "important_attribute_business_df = ...\n",
    "important_attribute_business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_3e\", important_attribute_business_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 3f\n",
    "\n",
    "At this point, you have had experience with manipulating data on Mongo, Postgres, and Pandas. In this question, we will provide you with three scenarios and using the lessons you've learned so far, please specify which of the three (Mongo, Postgres, or Pandas) would work best for this specific use case.\n",
    "\n",
    "#### Question 3fi\n",
    "\n",
    "You are doing a data journalism piece on college sports. You collect a list of colleges and for each collegiate sport program within that college, you find the budget assigned for that program. Each college may offer a different variety of sports programs. You have a choice between the following:\n",
    "\n",
    "A) Representing this data in JSON (e.g. \n",
    "```\n",
    "{\n",
    "    \"UC Berkeley\": {\n",
    "        \"football\": \"10000000\", \n",
    "        \"wrestling\": \"344582\", \n",
    "        ...}\n",
    "}\n",
    "```\n",
    ") and importing into Mongo.\n",
    "\n",
    "B) Representing this data as a schema in Postgres where the columns are the names of the sports.\n",
    "\n",
    "C) Representing this data as a dataframe in Pandas where the columns are the names of the sports.\n",
    "\n",
    "You would like to find the aggregate of budgets across different sports (average, sum, median, mode). What would be the best option for storing this data?\n",
    "\n",
    "**Set the value of `q3fi` to `'A'`, `'B'`, or `'C'`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q3fi = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3fii\n",
    "\n",
    "You would now like to investigate the effect of budgets on student-athlete scholarships. After doing some research, you find a dataset that contains a list of every single athlete at every single college and their sport and scholarship levels (this is a massive 10GB+ dataset with millions of rows). You find another dataset that contains a list of colleges, their sports programs, and the program budget. This is another massive dataset with hundreds of thousands of rows. You would like to perform an inner join between the two datasets on school and program so you can view each student-athlete's scholarship with their sport's budget. You have a choice between the following:\n",
    "\n",
    "A) Representing each dataset in JSON (e.g. \n",
    "```\n",
    "{\"athletes\": [\n",
    "    {\"Chase Garbers\": {\n",
    "        \"school\": \"UC Berkeley\", \n",
    "        \"scholarship\": \"full\", \n",
    "        \"sport\": \"football\", \n",
    "        ...\n",
    "        }\n",
    "    }, \n",
    "    ...\n",
    "]}\n",
    "```\n",
    "and \n",
    "```\n",
    "{\"schools\": [\n",
    "    {\"UC Berkeley\": {\n",
    "        \"football\": {\n",
    "            \"budget\": \"10000000\"\n",
    "         }, \n",
    "         ...\n",
    "         }\n",
    "    }, \n",
    "    ...\n",
    " ]}\n",
    " ```\n",
    "), importing into Mongo, and doing a join there.\n",
    "\n",
    "B) Representing this data as 2 schemas in Postgres where the columns for the first schema are \n",
    "[`student_name`, `school`, `sport`, `scholarship`] and for the second [`school`, `sport`, `budget`].\n",
    "\n",
    "C) Representing this data as 2 dataframes in Pandas with the same columns as Postgres.\n",
    "\n",
    "What would be the best option for storing this data?\n",
    "\n",
    "**Set the value of `q3fii` to `'A'`, `'B'`, or `'C'`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q3fii = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3fiii\n",
    "\n",
    "Finally, you are ready to start writing your article! You decide to focus on just the data from UC Berkeley. You have access to a dataset of just UC Berkeley athletes along with their sports and scholarship levels. The scholarship level data was improperly cleaned: some scholarships are recorded as strings \"full\", \"half\", or \"none\" and some are recorded as integer percentages 0-100. You would like to provide this data to your readers in a format that is susceptible to easy visualizations: e.g. graphs that show how many athletes have a full vs. half vs. no scholarship, which sports have the highest percentages of athletes with full scholarships etc. What is the best way to store this data for this purpose?\n",
    "\n",
    "A) Represent the dataset in JSON e.g.\n",
    "```\n",
    "{\"athletes\": [\n",
    "    {\n",
    "       \"Chase Garbers\": {\n",
    "         \"scholarship\": \"full\", \n",
    "         \"sport\": \"football\"\n",
    "       }\n",
    "    },\n",
    "    {\n",
    "        \"Danielle Vosk\": {\n",
    "          \"scholarship\": 25,\n",
    "          \"sport\": \"basketball\"\n",
    "        }\n",
    "    },\n",
    "    ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "B) Represent this data as a schema in Postgres where the columns are [`student_name`, `sport`, `scholarship`]\n",
    "\n",
    "C) Represent this data as a dataframe in Pandas with the same columns as Postgres.\n",
    "    \n",
    "**Set the value of `q3fiii` to `'A'`, `'B'`, or `'C'`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q3fiii = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fiii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Messy JSON\n",
    "\n",
    "Many of the queries you've seen or written thus far were relatively reliable: aggregating and collecting over fields\n",
    "that you know exist for sure. But the nature of Mongo documents is that they are inherently flexible and semi-structured. Not every document will share every single field! In this question, we will explore how Mongo handles these use cases using the `business` collection.\n",
    "\n",
    "### Question 4a\n",
    "\n",
    "Imagine you are in charge of managing your family reunion. You would like to book a private room at a restaurant.\n",
    "However, you would also like to optimize for chaos. You notice that there is an attribute called `RestaurantsGoodForGroups`. You would like to write a query that returns all restaurants that **do not** have the `RestaurantsGoodForGroups` attribute so that the trajectory of the reunion is determined by fate. (**Hint:** Search up the `$exists` keyword). \n",
    "\n",
    "How many restaurants do not have the `RestaurantsGoodForGroups` attribute? Ensure that your output for the autograder is the **number of restaurants that do not have the `RestaurantsGoodForGroups` attribute** stored in `q4a` as an integer.\n",
    "\n",
    "**Note:** You would like this list to consist solely of restaurants. We define a restaurant as a business that has the word `Restaurants` in the `categories` field. You should perform a similar `$text` `$search` as in Question 1d. **We will work with this definition for the rest of the Question 4 as well!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following text index may be useful!\n",
    "if 'categories_text' not in business.index_information():\n",
    "    business.create_index([('categories', TEXT)])\n",
    "\n",
    "q4a_cursor = ...\n",
    "q4a = len(list(q4a_cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_4a\", q4a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 4b\n",
    "\n",
    "Your relatives inform you that they would like to be at the restaurant when it opens to beat the crowds. Furthermore, after gathering their availabilities, most of your relatives would prefer for the meal to be on a Friday and for the start time of the meal to be between 5:00 - 6:59 PM (17:00-18:59). Find the number of restaurants that open on Fridays between 17:00-18:59 and store this in a variable labeled `q4b`.\n",
    "\n",
    "As a reminder, **in order for a business to be a restaurant, it must have `Restaurants` in its categories.**\n",
    "\n",
    "Also, be aware that **`hours` can either be a dictionary or `None`**. Depending on how you write your aggregation pipeline, you may or may not need to explicitly deal with this. Hint: Read the [\\$match](https://www.mongodb.com/docs/manual/reference/operator/aggregation/match/) documentation carefully. **What does it automatically filter out?**\n",
    "\n",
    "**Hint**:\n",
    "- Once you have the Friday hours of a particular restaurant, you only need to look at the first time listed. For example, suppose Restaurant A has Friday hours like so: `15:00-16:00`. Restaurant A opens at 15:00, which is outside the range from 17:00-18:59, inclusive. Thus, we would not want to include this in our count. This is because your relatives want to be **at the restaurant when it opens.**\n",
    "- Set up an aggregation pipeline using the [\\$set](https://www.mongodb.com/docs/manual/reference/operator/aggregation/set/) and [\\$match](https://www.mongodb.com/docs/manual/reference/operator/aggregation/match/) stage operators. You may also want to use the [\\$split](https://www.mongodb.com/docs/manual/reference/operator/aggregation/split/) operator to parse out the Friday hours as an integer and then use comparison operators to find the restaurants that are open during the specified time. Note that using dot notation for array indexing in aggregation pipelines may not work as expected, so we recommend using [\\$arrayElemAt](https://www.mongodb.com/docs/manual/reference/operator/aggregation/arrayElemAt/) operator.\n",
    "- You may assume that the open hour times are well formed, e.g. the format is HH:MM-HH:MM where HH represents the hour (from 0-23) and MM represents the minutes (from 0-59)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4b_cursor = ...\n",
    "q4b = len(list(q4b_cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_4b\", q4b)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 4c\n",
    "\n",
    "Some members of your family are vegetarian so you would like to only eat at restaurants with the `Vegetarian` category. \n",
    "However, the `categories` are stored as a single string! You would like to make it easy to access `Vegetarian` as a separate field. Write a query that does the following: for every category in `categories`, add a new document that contains the `ObjectId` of the restaurant (labeled `_id`), the name of the business (labeled `name`), and the category (labeled `category`).\n",
    "\n",
    "For example, a document \n",
    "```\n",
    "{\n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\", \n",
    "     ..., \n",
    "     categories\" : \"Salad, Vegetarian, Restaurants\"\n",
    "} \n",
    "```\n",
    "would become \n",
    "```\n",
    "{\n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\",\n",
    "    “category”: \"Salad\"\n",
    "}\n",
    "```\n",
    "and \n",
    "```\n",
    "{\n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\",\n",
    "    “category”: \"Vegetarian\"\n",
    "}\n",
    "```\n",
    "and\n",
    "```\n",
    "{\n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\",\n",
    "    “category”: \"Restaurants\"\n",
    "}\n",
    "```\n",
    "\n",
    "Finally, to ensure your output is consistent with the autograder, sort in ascending order by `name` and break ties on `category`. Save your pipeline to a variable called `q4c_pipeline`.\n",
    "\n",
    "**Hints:**\n",
    "- Before doing anything else, **make sure that you filter businesses to only include restaurants, as defined in Question 4a and 4b**\n",
    "- The `$unwind` operator may be helpful here. You can find the documentation [here](https://www.mongodb.com/docs/manual/reference/operator/aggregation/unwind/). Be sure to check what object type `$unwind` operates on and watch out to make sure you don't have any unnecessary space in the `category` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4c_pipeline = ...\n",
    "result_4c = list(business.aggregate(q4c_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_4c[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "result_4c = list(business.aggregate(q4c_pipeline))[:50]\n",
    "grading_util.save_results(\"result_4c\", result_4c);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 4d\n",
    "This change in representation has made it super easy to view all the vegetarian restaurants and count them without the use of an index; this is because we can now simply filter by whether or not `'Vegetarian'` is the value of the `category` field in our document! Write some code to count how many vegetarian restaurants are in our dataset—notice that there is only one more pipeline stage you'll need to add on top of the pipeline stages in Question 4c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q4d_pipeline = q4c_pipeline[:] # copy pipeline from Q4c\n",
    "\n",
    "extra_stage = ...\n",
    "q4d_pipeline.append(extra_stage)\n",
    "\n",
    "result_4d = list(business.aggregate(q4d_pipeline))\n",
    "\n",
    "veg_count = len(result_4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell!\n",
    "# You must run this cell before running the autograder.\n",
    "grading_util.save_results(\"result_4d\", veg_count)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Project 4.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook.\n",
    "\n",
    "**Please save your notebook before exporting (this is a good time to do it!)** Otherwise, we may not be able to export your written responses to `proj4.pdf`. We will not be accepting regrade requests for failure to render written responses.\n",
    "\n",
    "**For your submission on Gradescope, you will only need to submit the single `proj4.zip` file generated by the export cell.** Please ensure that your submission `proj4.zip` file includes `proj4.pdf`, `proj4.ipynb`, and `results.zip`. \n",
    "\n",
    "**Please ensure that public tests pass upon submission.** It is your responsibility to wait until the autograder finishes running. We will not be accepting regrade requests for submission issues.\n",
    "\n",
    "**Common submission issues:** You MUST submit the generated zip file to the autograder. However, Safari is known to automatically unzip files upon downloading. You can fix this by going into Safari preferences, and deselect the box with the text \"Open safe files after downloading\" under the \"General\" tab. If you experience issues with downloading via clicking on the link, you can also navigate to the project 3 directory within JupyterHub (remove `proj4.ipynb` from the url), and manually download the generated zip files. Please post on Ed if you encounter any other submission issues.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_util.prepare_submission_and_cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some cute pet photos from Data 101 students and staff for all your hard work!\n",
    "\n",
    "If you have pet photos to submit, you can upload them to [this Ed post](https://edstem.org/us/courses/63937/discussion/5425084).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/h0bgtw1IJe2uVeKe5hMJ0Ssf\" alt=\"gray cat on windowsill\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/9ygfWbUocYqiuDZE6CIsfdXJ\" alt=\"gray cat on desk\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/xY3ttl3FgNVl2hrY37mJBBt3\" alt=\"sleeping dog\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/4l4Y8SXIlE9bCmCq2VtNLeLJ\" alt=\"border collie next to tree\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/KeBFG4hReyeggsPIAstZoWyZ\" alt=\"orange cat on desk\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/hdTEmYpesthqbLhvaXd3wJuv\" alt=\"smiling akita dog\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/HX4fUThex0fvkUPog0AAOoqX\" alt=\"puppy in the snow\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/5cJZhXpRAn2wT3DhGQv9cpsJ\" alt=\"happy dog looking up at you\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/RYNels3EAOl3S4ebPJaDf7es\" alt=\"happy dog looking up at you\" width=\"200\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"https://static.us.edusercontent.com/files/QMJdSuSKKW4V3fxBfVdAcNvK\" alt=\"happy dog looking up at you\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(files=['results.zip'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1a = grading_util.load_results(\"result_1a\")[0]\n>>> \"-\" in result_1a and \":\" in result_1a\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1b = grading_util.load_results(\"result_1b\")[0]\n>>> 3 <= result_1b <= 4.5\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1c = grading_util.load_results(\"result_1c\")[0]\n>>> len(result_1c)\n31",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1c = grading_util.load_results(\"result_1c\")[0]\n>>> are_keys_correct = []\n>>> VALID_KEYS = set(['average_stars', 'city_state'])\n>>> for doc in result_1c:\n...     are_keys_correct.append(set(doc.keys()) == VALID_KEYS)\n>>> all(are_keys_correct) # checks that each document contains only valid keys: 'average_stars', 'city_state'\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1c = grading_util.load_results(\"result_1c\")[0]\n>>> average_stars = list(map(lambda doc: doc['average_stars'], result_1c))\n>>> all(average_stars[i] >= average_stars[i + 1] for i in range(len(average_stars) - 1)) # average_stars should be sorted in descending order\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1c = grading_util.load_results(\"result_1c\")[0]\n>>> city_states = list(map(lambda doc: doc['city_state'], result_1c))\n>>> sorted_docs = sorted(result_1c, key=lambda doc: (-doc['average_stars'], doc['city_state']))\n>>> expected_city_states = list(map(lambda doc: doc['city_state'], sorted_docs))\n>>> city_states == expected_city_states # city_states should be sorted by average_stars descending, city_state ascending\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1d": {
     "name": "q1d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1d = grading_util.load_results(\"result_1d\")[0]\n>>> 700 <= result_1d <= 800\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1e": {
     "name": "q1e",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_1e = grading_util.load_results(\"result_1e\")[0]\n>>> len(result_1e)\n7500",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1e = grading_util.load_results(\"result_1e\")[0]\n>>> num_trues = 0\n>>> for doc in result_1e:\n...     if doc['to_avoid'] == 'true':\n...         num_trues += 1\n>>> num_trues\n728",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_1e = grading_util.load_results(\"result_1e\")[0]\n>>> num_falses = 0\n>>> for doc in result_1e:\n...     if doc['to_avoid'] == 'false':\n...         num_falses += 1\n>>> num_falses\n6772",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1f": {
     "name": "q1f",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> difference = grading_util.load_results(\"result_1f\")[0]\n>>> 0 < difference <= 300000\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df.shape\n(10, 8)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> str(list(result_2b_df.columns))\n\"['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny', 'cool', 'text']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['review_id'].iloc[:5]\n0    000bviMESLXmlIFKDzCEfw\n1    003VeQn6SrVQS4sYHlc0gg\n2    00HovWV7VcZZPx5IleoeWA\n3    00pmZ82_w6Mpky6dl2jpiA\n4    021UtGruSN1RA5YRS92E7w\nName: review_id, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['user_id'].iloc[:5]\n0    f7LnyAbhP5OSXvv_xiuZhw\n1    ba8ZSYE11LVepGCxwP9Vpg\n2    Nzaq0bJcE3q_bRdFrsFRsA\n3    r7e-6OS8A_gE_0CTUjQR3g\n4    6cyn5sP2OCarYV02KWtuGQ\nName: user_id, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['business_id'].iloc[:5]\n0    SNuCspoI3HKcwJpZL5FcjQ\n1    rdS7hBBeukiX4Led9OT8sg\n2    Ln-8CbKGZGmF-GCqMoMcpA\n3    IDxaD_0_9TlWyKKXBFwjMA\n4    5HMXgD_gui5n0Tc_hadesg\nName: business_id, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['stars'].iloc[:5]\n0    3.0\n1    5.0\n2    4.0\n3    1.0\n4    3.0\nName: stars, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['useful'].iloc[:5]\n0    2\n1    0\n2    0\n3    0\n4    0\nName: useful, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['funny'].iloc[:5]\n0    0\n1    0\n2    0\n3    0\n4    0\nName: funny, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['cool'].iloc[:5]\n0    0\n1    0\n2    0\n3    0\n4    0\nName: cool, dtype: object",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> query_2b, result_2b_df = grading_util.load_results(\"result_2b\")\n>>> result_2b_df['text'].iloc[:5]\n0    I went there not having read any reviews.  Fro...\n1    Molana has one of the most tender Chicken Barg...\n2    This place is really new, but they were pretty...\n3    The food was great .. we had breakfast... the ...\n4    The atmosphere is great for drinks but I'd say...\nName: text, dtype: object",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2c": {
     "name": "q2c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_2c = grading_util.load_results(\"result_2c\")[0]\n>>> len(result_2c)\n5",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2c = grading_util.load_results(\"result_2c\")[0]\n>>> all(list(map(lambda doc: 'business_info' in doc.keys(), result_2c))) # business_info should be a field in all documents\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2c = grading_util.load_results(\"result_2c\")[0]\n>>> all(list(map(lambda doc: 'review_id' in doc.keys(), result_2c))) # review_id should be a field in all documents\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_2c = grading_util.load_results(\"result_2c\")[0]\n>>> all(list(map(lambda doc: 'business_id' in doc.keys(), result_2c))) # business_id should be a field in all documents\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3b_df = grading_util.load_results(\"result_3b\")[0]\n>>> result_3b_df.shape\n(50, 3)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3b_df = grading_util.load_results(\"result_3b\")[0]\n>>> str(list(result_3b_df.columns))\n\"['name_x', 'name_y', 'stars']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3b_df = grading_util.load_results(\"result_3b\")[0]\n>>> str(list(result_3b_df['name_x'].iloc[:25]))\n\"['081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza', '081 Wood Fired Pizza']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3b_df = grading_util.load_results(\"result_3b\")[0]\n>>> str(list(result_3b_df['name_y'].iloc[:25]))\n'[\\'081 Wood Fired Pizza\\', \"Abee\\'s Hi-Tech Automotive Service\", \\'Advance Auto Parts\\', \\'Alberta Abbey\\', \"Andy\\'s Frozen Custard\", \\'Angel New & Used Tires\\', \\'Appliance Guys, Inc\\', \\'Archery Games Boston\\', \\'Argo & Lehne Jewelers\\', \\'Artistic Threading Studio\\', \\'Austin General Dentistry\\', \\'Austin Strings\\', \\'B Street Coffee House\\', \\'BCC Limos\\', \\'Bambolina Restaurant\\', \\'Bar Rione\\', \\'Barley Swine\\', \\'BedMart Mattress Superstores\\', \\'Bernard Watch Co.\\', \\'Bertolli Floors\\', \\'Bicycle One\\', \\'Big Kahuna\\', \\'Bites & Bubbles\\', \\'Body In Motion\\', \\'Boxwood Biscuit\\']'",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3b_df = grading_util.load_results(\"result_3b\")[0]\n>>> str(list(result_3b_df['stars'].iloc[:25]))\n'[4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5, 4.5]'",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3c_df = grading_util.load_results(\"result_3c\")[0]\n>>> result_3c_df.shape\n(58, 2)",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3c_df = grading_util.load_results(\"result_3c\")[0]\n>>> str(list(result_3c_df.columns))\n\"['column_name', 'percent_missing']\"",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3c_df = grading_util.load_results(\"result_3c\")[0]\n>>> list(result_3c_df.loc[result_3c_df['column_name'] == '_id'][\"percent_missing\"])[0]\n0.0",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3c_df = grading_util.load_results(\"result_3c\")[0]\n>>> list(result_3c_df.loc[result_3c_df['column_name'] == 'attributes.RestaurantsTableService'][\"percent_missing\"])[0]\n89.1",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_3c_df = grading_util.load_results(\"result_3c\")[0]\n>>> list(result_3c_df.loc[result_3c_df['column_name'] == 'hours.Sunday'][\"percent_missing\"])[0]\n46.5",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 1 <= result_q3d <= 58\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3e": {
     "name": "q3e",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_3e_df = grading_util.load_results(\"result_3e\")[0]\n>>> result_3e_df.shape\n(1000, 39)",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3fi": {
     "name": "q3fi",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q3fi)\n1",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'A' <= q3fi <= 'C'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3fii": {
     "name": "q3fii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q3fii)\n1",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'A' <= q3fii <= 'C'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3fiii": {
     "name": "q3fiii",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q3fiii)\n1",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> 'A' <= q3fiii <= 'C'\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q4a = grading_util.load_results(\"result_4a\")[0]\n>>> 8200 <= q4a <= 8300\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q4b = grading_util.load_results(\"result_4b\")[0]\n>>> 3300 <= q4b <= 3400\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4c": {
     "name": "q4c",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> result_4c = grading_util.load_results(\"result_4c\")[0]\n>>> str(list(map(lambda doc: doc['name'], result_4c[:25])))\n'[\" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \" Gruby\\'s New York Deli\", \\'# 19\\', \\'# 19\\', \\'# 19\\', \\'#1 Express China Lee\\', \\'#1 Express China Lee\\', \\'#FROYO Premium Frozen Yogurt\\', \\'#FROYO Premium Frozen Yogurt\\', \\'#FROYO Premium Frozen Yogurt\\', \\'#FROYO Premium Frozen Yogurt\\', \\'& Juice\\', \\'& Juice\\', \\'& Juice\\', \\'& Juice\\', \\'& Juice\\', \\'& Juice Co.\\', \\'& Juice Co.\\', \\'& Juice Co.\\', \\'& Juice Co.\\']'",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> result_4c = grading_util.load_results(\"result_4c\")[0]\n>>> str(list(map(lambda doc: doc['category'], result_4c[:25])))\n\"['Caterers', 'Delis', 'Event Planning & Services', 'Kosher', 'Restaurants', 'Salad', 'Sandwiches', 'Food', 'Restaurants', 'Sandwiches', 'Chinese', 'Restaurants', 'American (New)', 'Food', 'Ice Cream & Frozen Yogurt', 'Restaurants', 'Food', 'Juice Bars & Smoothies', 'Restaurants', 'Vegan', 'Vegetarian', 'Food', 'Gluten-Free', 'Juice Bars & Smoothies', 'Restaurants']\"",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4d": {
     "name": "q4d",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> veg_count = grading_util.load_results(\"result_4d\")[0]\n>>> 1400 < veg_count < 1500\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
